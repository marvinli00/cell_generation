{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from utils.edm_utils import edm_clean_image_to_model_input, edm_model_output_to_x_0_hat\n",
    "\n",
    "import math\n",
    "from torch.distributions.normal import Normal\n",
    "from functools import partial\n",
    "import wandb\n",
    "import tqdm\n",
    "def compute_ess(w, dim=0):\n",
    "    ess = (w.sum(dim=dim))**2 / torch.sum(w**2, dim=dim)\n",
    "    return ess\n",
    "\n",
    "def compute_ess_from_log_w(log_w, dim=0):\n",
    "    return compute_ess(normalize_weights(log_w, dim=dim), dim=dim)\n",
    "\n",
    "def normalize_weights(log_weights, dim=0):\n",
    "    return torch.exp(normalize_log_weights(log_weights, dim=dim))\n",
    "\n",
    "def normalize_log_weights(log_weights, dim):\n",
    "    log_weights = log_weights - log_weights.max(dim=dim, keepdims=True)[0]\n",
    "    log_weights = log_weights - torch.logsumexp(log_weights, dim=dim, keepdims=True)\n",
    "    return log_weights\n",
    "def normalize_log_weights_everything(log_weights_list, dim):\n",
    "    return [normalize_log_weights(log_weights, dim) for log_weights in log_weights_list]\n",
    "\n",
    "def log_normal_density(sample, mean, var):\n",
    "    return Normal(loc=mean, scale=torch.sqrt(var)).log_prob(sample)\n",
    "def systematic_resampling(particles, weights):\n",
    "    \"\"\"\n",
    "    Perform systematic resampling on a set of particles based on their weights.\n",
    "    \n",
    "    Args:\n",
    "        particles (numpy.ndarray): Array of particles to be resampled (N x D)\n",
    "        weights (numpy.ndarray): Array of particle weights (N,)\n",
    "        \n",
    "    Returns:\n",
    "        tuple:\n",
    "            - numpy.ndarray: Resampled particles\n",
    "            - numpy.ndarray: New uniform weights\n",
    "            - numpy.ndarray: Indices of selected particles\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "    # Normalize weights\n",
    "    \n",
    "    def normalize_weights(weights):\n",
    "        return weights / torch.sum(weights)\n",
    "    \n",
    "    weights = normalize_weights(weights)\n",
    "    \n",
    "    # Calculate cumulative sum of weights\n",
    "    cumsum = torch.cumsum(weights, dim = 0)\n",
    "    \n",
    "    # Generate systematic noise (one random number)\n",
    "    u = torch.distributions.Uniform(low=0.0, high=1.0/N).sample()\n",
    "    #u = np.random.uniform(0, 1/N)\n",
    "    \n",
    "    # Generate points for systematic sampling\n",
    "    points = torch.zeros(N)\n",
    "    for i in range(N):\n",
    "        points[i] = u + i/N\n",
    "    \n",
    "    # Initialize arrays for results\n",
    "    indexes = torch.zeros(N, dtype=int)\n",
    "    cumsum = torch.cat([torch.tensor([0.0], device = self.device), cumsum])  # Add 0 at the beginning for easier indexing\n",
    "    \n",
    "    # Perform systematic resampling\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        while points[i] > cumsum[j+1]:\n",
    "            j += 1\n",
    "        indexes[i] = j\n",
    "        i += 1\n",
    "    \n",
    "    # Resample particles and reset weights\n",
    "    resampled_particles = particles[indexes]\n",
    "    #new_weights = torch.ones(N, device = self.device) / N\n",
    "    #log new_weights\n",
    "    new_weights = torch.zeros(N, device = self.device)\n",
    "    return resampled_particles, new_weights, indexes   \n",
    "    \n",
    "\n",
    "def get_xstart_var(alphas_cumprod_t, tausq_=0.05,var_type = 6):\n",
    "    \n",
    "    sigmasq_ = (1-alphas_cumprod_t) / alphas_cumprod_t\n",
    "    if var_type == 1:\n",
    "        return sigmasq_ \n",
    "    elif var_type == 2: # pseudoinverse-guided paper https://openreview.net/forum?id=9_gsMA8MRKQ \n",
    "        tausq_ = 1.0 \n",
    "        return (sigmasq_ * tausq_) / (sigmasq_ + tausq_)\n",
    "        #return (1 - alphas_cumprod_t) \n",
    "    elif var_type == 5: \n",
    "        tausq_ = 0.30 \n",
    "        return (sigmasq_ * tausq_) / (sigmasq_ + tausq_)\n",
    "    elif var_type == 3: # DPS paper https://arxiv.org/abs/2209.14687 \n",
    "        return None  \n",
    "    elif var_type == 4: # pseudoinverse-guided paper -- the actual implementation, see their Alg.1 \n",
    "        return beta_t  / np.sqrt(alphas_cumprod_t) \n",
    "    elif var_type == 6: # freely specify tausq_\n",
    "        tausq_ = tausq_ \n",
    "        return (sigmasq_ * tausq_) / (sigmasq_ + tausq_)\n",
    "def compute_ess_softmax(log_weights):\n",
    "    #softmax\n",
    "    weights = torch.nn.functional.softmax(log_weights, dim = 0)\n",
    "    return 1/torch.sum(weights**2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/anaconda3/envs/image_tds/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import project modules\n",
    "from models.unet import create_unet_model, load_vae, load_classifier, load_clip_model, CustomUNetWithEmbeddings\n",
    "from schedulers.edm_scheduler import create_edm_scheduler\n",
    "from utils.edm_utils import edm_clean_image_to_model_input, edm_model_output_to_x_0_hat\n",
    "from config.default_config import EDM_CONFIG\n",
    "from models.clip_image_encoder import OpenCLIPVisionEncoder\n",
    "from data.dataset import FullFieldDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set precision (you can adjust this based on your hardware)\n",
    "weight_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# # Define paths to models and checkpoints\n",
    "# model_path = \"/path/to/your/trained/model\"  # Update this to your model checkpoint path\n",
    "# vae_path = \"/scratch/groups/emmalu/marvinli/twisted_diffusion/stable-diffusion-3.5-large-turbo/vae\"\n",
    "# classifier_path = \"/scratch/groups/emmalu/marvinli/twisted_diffusion/checkpoints_classifier/model_epoch_7.pth\"\n",
    "# clip_model_path = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "\n",
    "\n",
    "# Define paths to models and checkpoints\n",
    "model_path = \"/home/pc/Documents/twisted_diffusion/two_labels_latent_diffusion_edm_silu_less_cross_attn/checkpoint-200000\"  # Update this to your model checkpoint path\n",
    "vae_path = \"/home/pc/Documents/twisted_diffusion_helper_model/vae\"\n",
    "classifier_path = \"/home/pc/Documents/twisted_diffusion_helper_model/checkpoints_classifier/model_epoch_7.pth\"\n",
    "clip_model_path = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "\n",
    "# Set EDM parameters\n",
    "sigma_min = EDM_CONFIG[\"SIGMA_MIN\"]\n",
    "sigma_max = EDM_CONFIG[\"SIGMA_MAX\"]\n",
    "sigma_data = EDM_CONFIG[\"SIGMA_DATA\"]\n",
    "rho = EDM_CONFIG[\"RHO\"]\n",
    "\n",
    "# Set sampling parameters\n",
    "num_inference_steps = 1000\n",
    "guidance_scale = 0  # Higher values increase adherence to the conditioning\n",
    "batch_size = 1\n",
    "image_size = 1  # Size of the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'act_fn': 'silu', 'addition_embed_type': None, 'addition_embed_type_num_heads': 64, 'addition_time_embed_dim': None, 'attention_head_dim': 8, 'attention_type': 'default', 'center_input_sample': False, 'class_embeddings_concat': False, 'conv_in_kernel': 3, 'conv_out_kernel': 3, 'cross_attention_norm': None, 'downsample_padding': 1, 'dropout': 0.0, 'dual_cross_attention': False, 'encoder_hid_dim': None, 'encoder_hid_dim_type': None, 'flip_sin_to_cos': True, 'freq_shift': 0, 'mid_block_only_cross_attention': None, 'mid_block_scale_factor': 1, 'norm_eps': 1e-05, 'norm_num_groups': 32, 'num_attention_heads': None, 'num_class_embeds': None, 'only_cross_attention': False, 'resnet_out_scale_factor': 1.0, 'resnet_skip_time_act': False, 'resnet_time_scale_shift': 'default', 'reverse_transformer_layers_per_block': None, 'time_cond_proj_dim': None, 'time_embedding_act_fn': None, 'time_embedding_dim': None, 'time_embedding_type': 'positional', 'timestep_post_act': None, 'transformer_layers_per_block': 1, 'upcast_attention': False, 'use_linear_projection': False} were passed to CustomUNetWithEmbeddings, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_unet_model(resolution=image_size)\n",
    "from diffusers import UNet2DConditionModel\n",
    "model = CustomUNetWithEmbeddings.from_pretrained(model_path, subfolder=\"unet\")\n",
    "# # Load model checkpoint\n",
    "# try:\n",
    "#     # Try loading state dict directly\n",
    "#     state_dict = torch.load(os.path.join(model_path, \"unet\", \"diffusion_pytorch_model.bin\"), map_location=\"cpu\")\n",
    "#     model.load_state_dict(state_dict)\n",
    "# except:\n",
    "#     # Fallback to loading from checkpoint file\n",
    "#     checkpoint = torch.load(os.path.join(model_path, \"checkpoint.pt\"), map_location=\"cpu\")\n",
    "#     if \"model\" in checkpoint:\n",
    "#         model.load_state_dict(checkpoint[\"model\"])\n",
    "#     else:\n",
    "#         model.load_state_dict(checkpoint)\n",
    "\n",
    "# Move model to device and set to evaluation mode\n",
    "model.to(device)\n",
    "model.to(weight_dtype)\n",
    "model.eval()\n",
    "\n",
    "# Load VAE\n",
    "class DummyAccelerator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "dummy_accelerator = DummyAccelerator(device)\n",
    "vae = load_vae(vae_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "# Load scheduler\n",
    "scheduler = create_edm_scheduler(\n",
    "    sigma_min=sigma_min,\n",
    "    sigma_max=sigma_max,\n",
    "    sigma_data=sigma_data,\n",
    "    num_train_timesteps=1000,\n",
    "    prediction_type=\"sample\"\n",
    ")\n",
    "\n",
    "# Move scheduler sigmas to device\n",
    "scheduler.sigmas = scheduler.sigmas.to(device)\n",
    "\n",
    "# Load CLIP model (optional)\n",
    "clip_model = load_clip_model(clip_model_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "# Load classifier (optional)\n",
    "classifier = load_classifier(classifier_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_latent_sample(vae, images, weight_dtype=torch.float32):\n",
    "    \"\"\"Encode images to latent space using VAE\"\"\"\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(images).latent_dist.sample()\n",
    "    return latent\n",
    "\n",
    "def prepare_model_inputs(gt_images_latent, cond_images_latent, cell_line, label, dropout_prob=0.0, weight_dtype=torch.float32, encoder_hidden_states=None):\n",
    "    \"\"\"Prepare model inputs including latents and conditioning\"\"\"\n",
    "    # Combine protein and cell line for conditioning\n",
    "    batch_size = cond_images_latent.shape[0]\n",
    "    \n",
    "    # Create dropout mask for classifier-free guidance\n",
    "    dropout_mask = torch.rand(batch_size) > dropout_prob\n",
    "    \n",
    "    # Create full label tensor including cell line and label\n",
    "    total_label = torch.cat([cell_line, label], dim=1).to(weight_dtype)\n",
    "    \n",
    "    # Create a clean latent by combining ground truth and conditioning latents\n",
    "    clean_images = torch.cat([gt_images_latent, cond_images_latent], dim=1)\n",
    "    \n",
    "    return clean_images, total_label, encoder_hidden_states, dropout_mask\n",
    "\n",
    "def decode_latents(vae, latents, scaling_factor=4.0):\n",
    "    \"\"\"Decode latent samples to images using VAE\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Scale latents\n",
    "        latents = latents * 4 / vae.scaling_factor\n",
    "        \n",
    "        # Decode the latents to images\n",
    "        images = vae.decode(latents).sample\n",
    "        \n",
    "        # Normalize images to [0, 1] range\n",
    "        images = (images / 2 + 0.5).clamp(0, 1)\n",
    "        \n",
    "    return images\n",
    "\n",
    "def prepare_conditioning(clip_image=None, cell_line=None, label=None, batch_size=1, device=\"cuda\", weight_dtype=torch.float32):\n",
    "    \"\"\"Prepare conditioning inputs\"\"\"\n",
    "    # Process CLIP image if provided\n",
    "    if clip_image is not None:\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = clip_model(clip_image)\n",
    "    else:\n",
    "        # Create empty encoder hidden states\n",
    "        encoder_hidden_states = torch.zeros((batch_size, 196, 768), device=device, dtype=weight_dtype)\n",
    "    \n",
    "    # Set up cell line and label conditioning\n",
    "    if cell_line is None:\n",
    "        # Create a one-hot vector for cell line (assuming 40 cell lines)\n",
    "        cell_line = torch.zeros((batch_size, 40), device=device, dtype=weight_dtype)\n",
    "        cell_line[:, 0] = 1.0  # Set first cell line as default\n",
    "    \n",
    "    if label is None:\n",
    "        # Create a one-hot vector for label (assuming 13348 labels)\n",
    "        label = torch.zeros((batch_size, 13348), device=device, dtype=weight_dtype)\n",
    "        label[:, 0] = 1.0  # Set first label as default\n",
    "    \n",
    "    total_label = torch.cat([cell_line, label], dim=1)\n",
    "    \n",
    "    return encoder_hidden_states, total_label\n",
    "\n",
    "# def plot_images(images, row_title=None, **kwargs):\n",
    "#     \"\"\"Plot a grid of images\"\"\"\n",
    "#     if not isinstance(images, list):\n",
    "#         images = [images]\n",
    "    \n",
    "#     num_images = len(images)\n",
    "#     fig, axs = plt.subplots(1, num_images, figsize=(12, 12 // num_images))\n",
    "    \n",
    "#     if num_images == 1:\n",
    "#         axs = [axs]\n",
    "    \n",
    "#     for i, img in enumerate(images):\n",
    "#         if isinstance(img, torch.Tensor):\n",
    "#             img = img.detach().cpu().numpy()\n",
    "        \n",
    "#         # Handle different shapes and channel configurations\n",
    "#         if img.ndim == 4 and img.shape[0] == 1:  # [1, C, H, W]\n",
    "#             img = img[0]\n",
    "        \n",
    "#         if img.shape[0] == 3 or img.shape[0] == 1:  # [C, H, W]\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "        \n",
    "#         if img.shape[-1] == 1:  # Single channel\n",
    "#             img = img.squeeze(-1)\n",
    "#             axs[i].imshow(img, cmap='gray')\n",
    "#         else:  # RGB\n",
    "#             axs[i].imshow(img)\n",
    "        \n",
    "#         axs[i].set_xticks([])\n",
    "#         axs[i].set_yticks([])\n",
    "    \n",
    "#     if row_title is not None:\n",
    "#         fig.suptitle(row_title)\n",
    "#     #save the figure\n",
    "#     plt.savefig(f\"{row_title}.png\")\n",
    "#     plt.tight_layout()\n",
    "#     #plt.show()\n",
    "    \n",
    "def plot_images(images, row_title=None, **kwargs):\n",
    "    \"\"\"Plot a grid of images and save to file without displaying\"\"\"\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "    \n",
    "    num_images = len(images)\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(12, 12 // num_images))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().cpu().numpy()\n",
    "        \n",
    "        # Handle different shapes and channel configurations\n",
    "        if img.ndim == 4 and img.shape[0] == 1:  # [1, C, H, W]\n",
    "            img = img[0]\n",
    "        \n",
    "        if img.shape[0] == 3 or img.shape[0] == 1:  # [C, H, W]\n",
    "            img = img.transpose(1, 2, 0)\n",
    "        \n",
    "        if img.shape[-1] == 1:  # Single channel\n",
    "            img = img.squeeze(-1)\n",
    "            axs[i].imshow(img, cmap='gray')\n",
    "        else:  # RGB\n",
    "            axs[i].imshow(img)\n",
    "        \n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "    \n",
    "    if row_title is not None:\n",
    "        fig.suptitle(row_title)\n",
    "    \n",
    "    # Save the figure with appropriate filename\n",
    "    filename = f\"{row_title}.png\" if row_title else \"plot.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # Close the figure to prevent display and free memory\n",
    "    \n",
    "    \n",
    "def save_image(image, output_filename=\"output.png\", **kwargs):\n",
    "    \"\"\"Save a single image to PNG file without using matplotlib\"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Handle torch tensor\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.detach().cpu().numpy()\n",
    "    \n",
    "    # Handle different shapes and channel configurations\n",
    "    if image.ndim == 4 and image.shape[0] == 1:  # [1, C, H, W]\n",
    "        image = image[0]\n",
    "    \n",
    "    if image.shape[0] == 3 or image.shape[0] == 1:  # [C, H, W]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "    \n",
    "    # Handle single channel images\n",
    "    if image.shape[-1] == 1:  # Single channel\n",
    "        image = image.squeeze(-1)\n",
    "        # For grayscale, just use L mode\n",
    "        pil_mode = 'L'\n",
    "    else:  # RGB\n",
    "        pil_mode = 'RGB'\n",
    "    \n",
    "    # Ensure values are in valid range for PIL\n",
    "    if image.max() <= 1.0:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    else:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Create and save the PIL image\n",
    "    if not output_filename.endswith('.png'):\n",
    "        output_filename += '.png'\n",
    "        \n",
    "    pil_img = Image.fromarray(image, mode=pil_mode)\n",
    "    pil_img.save(output_filename)\n",
    "    \n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twisting_mse(x_0_hat, mask, sigma_dt, number_of_particles):\n",
    "    score = 0\n",
    "    normalized_variance = get_xstart_var(sigma_dt, var_type = 6, tausq_ = 0.012)\n",
    "\n",
    "    #Gaussian log probability: -||x-μ||²/(2σ²)\n",
    "    score_i = -torch.sum((x_0_hat[:,:16,:,:] - mask[None,None,None,:]) ** 2, dim=(2,3)) / (2*normalized_variance)\n",
    "\n",
    "    # Log the mean squared distance for monitoring/debugging\n",
    "    #self.run.log({f\"distances_of_motif\": ((ts_com_zero - motif_target_cat[None,None,None,:])**2).mean()})\n",
    "    #write to a\n",
    "    # Add this motif's score to the total score\n",
    "    # Multiple motifs' scores are summed, giving equal weight to each motif\n",
    "    score = score + score_i\n",
    "    score_log_proob_given_motif = torch.logsumexp(score, dim=0) - torch.log(torch.tensor(number_of_particles, device=device))\n",
    "    \n",
    "    \n",
    "    return score_log_proob_given_motif.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_edm(\n",
    "    model,\n",
    "    scheduler,\n",
    "    twisting_target,\n",
    "    batch_size=1,\n",
    "    image_size=32,\n",
    "    number_of_particles=16,\n",
    "    \n",
    "    num_inference_steps=50,\n",
    "    condition_latent=None,  # Optional conditioning latent\n",
    "    encoder_hidden_states=None,  # CLIP hidden states\n",
    "    class_labels=None,  # Class labels for conditioning\n",
    "    \n",
    "    protein_labels=None,\n",
    "    cell_line_labels=None,\n",
    "    \n",
    "    guidance_scale=1.0,  # Scale for classifier-free guidance\n",
    "    generator=None,\n",
    "    output_type=\"latent\",  # \"latent\" or \"pt\"\n",
    "    \n",
    "    # Add new parameters from EDMEulerScheduler.step method\n",
    "    s_churn=0.0,\n",
    "    s_tmin=0.0,\n",
    "    s_tmax=float(\"inf\"),\n",
    "    s_noise=1.0,\n",
    "    device = None,\n",
    "    weight_dtype = None\n",
    "):      \n",
    "    batch_size = number_of_particles\n",
    "    protein_labels = protein_labels.repeat(batch_size, 1)\n",
    "    cell_line_labels = cell_line_labels.repeat(batch_size, 1)\n",
    "    encoder_hidden_states = encoder_hidden_states.repeat(batch_size, 1, 1)\n",
    "    \n",
    "    # Create random noise for the ground truth part\n",
    "    latent_channels = 32\n",
    "    gt_noise = torch.randn(\n",
    "        (batch_size, latent_channels, image_size, image_size),\n",
    "        generator=generator,\n",
    "        device=device,\n",
    "        dtype=weight_dtype\n",
    "        )\n",
    "    latents = gt_noise * scheduler.sigmas[0].to(device)\n",
    "    progress_bar = tqdm(range(num_inference_steps))\n",
    "    progress_bar.set_description(\"Sampling\")\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps = scheduler.timesteps\n",
    "\n",
    "    \n",
    "    #log \n",
    "    log_proposal = log_normal_density(latents, torch.tensor(0, device = device), scheduler.sigmas[0].item()**2*torch.tensor(1, device = device))\n",
    "    #sum over all dim except batch and particle\n",
    "    log_proposal = log_proposal.sum(dim = (1,2,3))\n",
    "    \n",
    "    log_proposal_tracker = []\n",
    "    log_proposal_tracker.append(log_proposal)\n",
    "    \n",
    "    #for ess, tracing weights\n",
    "    ess_tracker = []\n",
    "    #ess_tracker.append(\n",
    "    \n",
    "    #initialize weights\n",
    "    log_w_prev_accumulated = torch.log(torch.ones_like(log_proposal, device = device))\n",
    "    \n",
    "\n",
    "    # Define steps\n",
    "    \n",
    "    \n",
    "    torch_default_dtype = torch.get_default_dtype()\n",
    "\n",
    "    # Iterate\n",
    "    for i, t in enumerate(progress_bar):\n",
    "\n",
    "        sigma = scheduler.sigmas[i]\n",
    "        sigma_next = scheduler.sigmas[i + 1] if i < len(scheduler.sigmas) - 1 else torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Expand sigma for broadcasting\n",
    "        sigma_expanded = sigma.expand(batch_size).to(device)\n",
    "        sigma_view = sigma_expanded.view(-1, 1, 1, 1).double()\n",
    "        \n",
    "        # Calculate gamma for stochastic sampling (from the step method)\n",
    "        gamma = min(s_churn / (len(scheduler.sigmas) - 1), 2**0.5 - 1) if s_tmin <= sigma <= s_tmax else 0.0\n",
    "        sigma_hat = sigma * (gamma + 1)\n",
    "        sigma_hat_view = sigma_hat.view(-1, 1, 1, 1).double().to(device)\n",
    "\n",
    "        \n",
    "        # Add noise if gamma > 0 (s_churn is active) - implements stochastic sampling\n",
    "        if gamma > 0:\n",
    "            noise = torch.randn((batch_size, latent_channels, image_size, image_size), generator=generator, device=device, dtype=latents.dtype)\n",
    "            eps = noise * s_noise\n",
    "            latents = latents + eps * (sigma_hat**2 - sigma**2) ** 0.5\n",
    "        \n",
    "        latents = latents.double()\n",
    "        \n",
    "        # Combine latents with condition latent\n",
    "        combined_latent = latents\n",
    "        \n",
    "        # Prepare input with noise according to EDM formulation\n",
    "        model_input, timestep_input = edm_clean_image_to_model_input(combined_latent, sigma_hat_view)\n",
    "        timestep_input = timestep_input.squeeze()\n",
    "        \n",
    "        # For classifier-free guidance, we need to do two forward passes:\n",
    "        # one with the conditioning and one without\n",
    "\n",
    "        # Regular conditional forward pass\n",
    "        model.to(weight_dtype)\n",
    "        model_input = model_input.to(weight_dtype)\n",
    "        timestep_input = timestep_input.to(weight_dtype)\n",
    "        \n",
    "        if encoder_hidden_states is not None:\n",
    "            encoder_hidden_states = encoder_hidden_states.to(weight_dtype)\n",
    "        \n",
    "        model_output = model(\n",
    "            model_input,\n",
    "            timestep_input,\n",
    "            protein_labels=protein_labels,\n",
    "            cell_line_labels=cell_line_labels,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        ).sample\n",
    "    \n",
    "        # Convert model output to denoised latent (x0 prediction)\n",
    "        #find E[x_0|x_t, t] unconditional\n",
    "        untwisted_predicted_x_start = edm_model_output_to_x_0_hat(combined_latent, sigma_hat_view, model_output.double())\n",
    "\n",
    "        step_sigma = sigma - sigma_next\n",
    "        \n",
    "        #compute log p(y|x_t, t) := log N(y; x_0, sigma_t^2 I)\n",
    "        log_prob = twisting_mse(untwisted_predicted_x_start, twisting_target, step_sigma, number_of_particles).squeeze()\n",
    "        \n",
    "\n",
    "        grad_pk_with_respect_to_x_t = torch.autograd.grad(log_prob.mean(), combined_latent)[0]\n",
    "        #rescale mean back to the original scale\n",
    "        grad_pk_with_respect_to_x_t = grad_pk_with_respect_to_x_t*combined_latent.shape[0]\n",
    "        with torch.no_grad():\n",
    "            alpha = 0.012\n",
    "            # |grad_pk_with_respect_to_x_t|_F\n",
    "            norm_grad = grad_pk_with_respect_to_x_t.norm()\n",
    "            #regularize gradient to prevent gradient explosion\n",
    "            grad_pk_with_respect_to_x_t = grad_pk_with_respect_to_x_t*alpha*norm_grad/(alpha+norm_grad)\n",
    "        #if i >= 50:            \n",
    "        twisted_predicted_x_start = untwisted_predicted_x_start + grad_pk_with_respect_to_x_t\n",
    "        # #find u(x_t-1) = E[x_t-1|x_t, (x_0, y),t]\n",
    "        # trans_mean = (posterior_mean_coef1 * twisted_predicted_x_start + \n",
    "        #                 posterior_mean_coef2 * ts.trans)\n",
    "        \n",
    "        denoised_twisted= twisted_predicted_x_start\n",
    "        denoised_untwisted = untwisted_predicted_x_start\n",
    "        step_size = step_sigma / sigma\n",
    "        \n",
    "        direction_twisted = (denoised_twisted - latents) / sigma_view\n",
    "        latents_twisted = latents + step_size.item() * sigma_view * direction_twisted\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            direction_untwisted = (denoised_untwisted - latents) / sigma_view\n",
    "            latents_untwisted = latents + step_size.item() * sigma_view * direction_untwisted\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #get p~^(t+1)_k\n",
    "            log_proposal = log_proposal_tracker.pop().squeeze()\n",
    "            log_proposal_tracker.append(log_prob)\n",
    "            #get p~^(t)_k\n",
    "            log_potential_xt = log_prob\n",
    "        \n",
    "\n",
    "            # Find p(xt_k|xt+1_k) - the reverse transition probability\n",
    "            log_reverse_transition = self.log_normal_density(latents, latents_untwisted, step_sigma.pow(2)).sum(dim = (1,2))\n",
    "\n",
    "            # Find p~(xt_k|xt+1_k,y) - the twisted reverse transition\n",
    "            log_twisted_transition = self.log_normal_density(latents, latents_twisted, step_sigma.pow(2)).sum(dim = (1,2))\n",
    "            temp = log_reverse_transition - log_twisted_transition\n",
    "            # Calculate importance weight\n",
    "            log_target = log_reverse_transition + log_potential_xt - log_twisted_transition  \n",
    "            #unnormalize log_w\n",
    "            log_w = log_target - log_proposal\n",
    "            log_w_accumulated = log_w + log_w_prev_accumulated\n",
    "                \n",
    "            ess =  compute_ess_from_log_w(log_w_accumulated)\n",
    "            \n",
    "            # ess = self.compute_ess(log_w_accumulated)\n",
    "            ess_tracker.append(ess.detach().cpu().numpy())\n",
    "            self.run.log({\"ess\": ess})\n",
    "            #resample when ess is too low (50% of num_samples)\n",
    "            if ess < 0.5*number_of_particles:\n",
    "                weights = torch.nn.functional.softmax(log_w_accumulated, dim = 0)\n",
    "                #resample\n",
    "                trans, log_w_prev_accumulated, indexes = self.systematic_resampling(trans, weights)\n",
    "                log_proposal_tracker[0] = log_proposal_tracker[0][indexes]\n",
    "            else:\n",
    "                #log_w = normalize_log_weights(log_w, dim=0)\n",
    "                log_w_prev_accumulated = normalize_log_weights(log_w_accumulated, dim=0) + torch.log(torch.tensor(number_of_particles, device=self.device))\n",
    "                # Compute rotations\n",
    "            latents = latents_twisted\n",
    "\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1000 [00:05<?, ?it/s]\n",
      "  0%|          | 0/11129 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (256) at non-singleton dimension 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m twisting_target \u001b[38;5;241m=\u001b[39m gt_images[:,:\u001b[38;5;241m16\u001b[39m,:,:]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Sample from the model\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m generated_latents \u001b[38;5;241m=\u001b[39m \u001b[43msample_edm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_particles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtwisting_target\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtwisting_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition_latent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotein_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprotein_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_line_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell_line\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms_churn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_dtype\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Decode the latents to images\u001b[39;00m\n\u001b[1;32m     69\u001b[0m vae\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[15], line 127\u001b[0m, in \u001b[0;36msample_edm\u001b[0;34m(model, scheduler, twisting_target, batch_size, image_size, number_of_particles, num_inference_steps, condition_latent, encoder_hidden_states, class_labels, protein_labels, cell_line_labels, guidance_scale, generator, output_type, s_churn, s_tmin, s_tmax, s_noise, device, weight_dtype)\u001b[0m\n\u001b[1;32m    124\u001b[0m step_sigma \u001b[38;5;241m=\u001b[39m sigma \u001b[38;5;241m-\u001b[39m sigma_next\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#compute log p(y|x_t, t) := log N(y; x_0, sigma_t^2 I)\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mtwisting_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43muntwisted_predicted_x_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwisting_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_sigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_particles\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    130\u001b[0m grad_pk_with_respect_to_x_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(log_prob\u001b[38;5;241m.\u001b[39mmean(), combined_latent)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#rescale mean back to the original scale\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mtwisting_mse\u001b[0;34m(x_0_hat, mask, sigma_dt, number_of_particles)\u001b[0m\n\u001b[1;32m      3\u001b[0m normalized_variance \u001b[38;5;241m=\u001b[39m get_xstart_var(sigma_dt, var_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m, tausq_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.012\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Gaussian log probability: -||x-μ||²/(2σ²)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m score_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum((\u001b[43mx_0_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnormalized_variance)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Log the mean squared distance for monitoring/debugging\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#self.run.log({f\"distances_of_motif\": ((ts_com_zero - motif_target_cat[None,None,None,:])**2).mean()})\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#write to a\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add this motif's score to the total score\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Multiple motifs' scores are summed, giving equal weight to each motif\u001b[39;00m\n\u001b[1;32m     13\u001b[0m score \u001b[38;5;241m=\u001b[39m score \u001b[38;5;241m+\u001b[39m score_i\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (256) at non-singleton dimension 6"
     ]
    }
   ],
   "source": [
    "test_dataset = FullFieldDataset(\n",
    "        data_root='/home/pc/Documents/twisted_diffusion_helper_model/test_images',\n",
    "        label_dict='/home/pc/Documents/twisted_diffusion_helper_model/antibody_map.pkl',\n",
    "        annotation_dict='/home/pc/Documents/twisted_diffusion_helper_model/annotation_map.pkl'\n",
    "    )\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    ")\n",
    "# Get a batch of test data\n",
    "#batch = next(iter(test_dataloader))\n",
    "from tqdm import tqdm\n",
    "    # Generate samples\n",
    "weight_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "clip_model.to(weight_dtype)\n",
    "count = 0\n",
    "for batch in tqdm(test_dataloader):\n",
    "\n",
    "    # Show conditioning image\n",
    "    cond_images = batch[\"cond_image\"].to(weight_dtype).to(device)\n",
    "    clip_images = batch[\"clip_image\"].to(weight_dtype).to(device)\n",
    "    gt_images = batch[\"gt_image\"].to(weight_dtype).to(device)\n",
    "    # Encode conditioning image to latent space\n",
    "    with torch.no_grad():\n",
    "        #cond_images_latent = prepare_latent_sample(vae, cond_images.repeat(1, 3, 1, 1), weight_dtype)\n",
    "        encoder_hidden_states = clip_model(clip_images)\n",
    "        \n",
    "    # Prepare cell_line and label conditioning\n",
    "    cell_line = batch[\"cell_line\"].to(device).long()\n",
    "    protein_label = batch[\"label\"].to(device).long()\n",
    "    #one hot encoding\n",
    "    #cell_line = torch.nn.functional.one_hot(cell_line, num_classes=40)\n",
    "    #label = torch.nn.functional.one_hot(label, num_classes=13348)\n",
    "\n",
    "    #total_label = torch.cat([cell_line, label], dim=1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    num_inference_steps=1000\n",
    "    with torch.no_grad():\n",
    "        # Set random seed for reproducibility\n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "        twisting_target = gt_images[:,:16,:,:]\n",
    "        # Sample from the model\n",
    "        generated_latents = sample_edm(\n",
    "            model=model,\n",
    "            scheduler=scheduler,\n",
    "            batch_size=1,\n",
    "            number_of_particles=2,\n",
    "            twisting_target = twisting_target,\n",
    "            image_size=32,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            condition_latent=None,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            class_labels=None,\n",
    "            protein_labels = protein_label,\n",
    "            cell_line_labels = cell_line,\n",
    "            guidance_scale=0,\n",
    "            generator=generator,\n",
    "            output_type=\"latent\",\n",
    "            s_churn = 0,\n",
    "            device = device,\n",
    "            weight_dtype = weight_dtype\n",
    "        )\n",
    "        \n",
    "        # Decode the latents to images\n",
    "        vae.to(torch.float32)\n",
    "        generated_latents = generated_latents.to(torch.float32)\n",
    "        generated_images_gt = decode_latents(vae, generated_latents[:,:16,:,:])\n",
    "        generated_images_cond = decode_latents(vae, generated_latents[:,16:,:,:])\n",
    "    \n",
    "        # Display conditioning image\n",
    "    for i in range(batch_size):\n",
    "        save_image(cond_images[i].cpu().float()*0.5+0.5, output_filename=f\"generated_images/Testset_Conditioning Image_{count}\")\n",
    "        save_image(generated_images_gt[i].cpu(), output_filename=f\"generated_images/Generated Ground Truth Image_{count}\")\n",
    "        save_image(gt_images[i].cpu().float()*0.5+0.5, output_filename=f\"generated_images/Testset_Ground Truth Image_{count}\")\n",
    "        save_image(generated_images_cond[i].cpu(), output_filename=f\"generated_images/Generated Conditioning Image_{count}\")\n",
    "        count += 1\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_tds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
