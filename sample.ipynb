{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twisted Diffusion Model Sampler\n",
    "\n",
    "This notebook provides a sampler for the trained diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import project modules\n",
    "from models.unet import create_unet_model, load_vae, load_classifier, load_clip_model, CustomUNetWithEmbeddings\n",
    "from schedulers.edm_scheduler import create_edm_scheduler\n",
    "from utils.edm_utils import edm_clean_image_to_model_input, edm_model_output_to_x_0_hat\n",
    "from config.default_config import EDM_CONFIG\n",
    "from models.clip_image_encoder import OpenCLIPVisionEncoder\n",
    "from data.dataset import FullFieldDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up device and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set precision (you can adjust this based on your hardware)\n",
    "weight_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# # Define paths to models and checkpoints\n",
    "# model_path = \"/path/to/your/trained/model\"  # Update this to your model checkpoint path\n",
    "# vae_path = \"/scratch/groups/emmalu/marvinli/twisted_diffusion/stable-diffusion-3.5-large-turbo/vae\"\n",
    "# classifier_path = \"/scratch/groups/emmalu/marvinli/twisted_diffusion/checkpoints_classifier/model_epoch_7.pth\"\n",
    "# clip_model_path = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "\n",
    "\n",
    "# Define paths to models and checkpoints\n",
    "model_path = \"/home/pc/Documents/twisted_diffusion/two_labels_latent_diffusion_edm_silu_less_cross_attn/checkpoint-200000\"  # Update this to your model checkpoint path\n",
    "vae_path = \"/home/pc/Documents/twisted_diffusion_helper_model/vae\"\n",
    "classifier_path = \"/home/pc/Documents/twisted_diffusion_helper_model/checkpoints_classifier/model_epoch_7.pth\"\n",
    "clip_model_path = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "\n",
    "# Set EDM parameters\n",
    "sigma_min = EDM_CONFIG[\"SIGMA_MIN\"]\n",
    "sigma_max = EDM_CONFIG[\"SIGMA_MAX\"]\n",
    "sigma_data = EDM_CONFIG[\"SIGMA_DATA\"]\n",
    "rho = EDM_CONFIG[\"RHO\"]\n",
    "\n",
    "# Set sampling parameters\n",
    "num_inference_steps = 100\n",
    "guidance_scale = 7.5  # Higher values increase adherence to the conditioning\n",
    "batch_size = 64\n",
    "image_size = 32  # Size of the generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'act_fn': 'silu', 'addition_embed_type': None, 'addition_embed_type_num_heads': 64, 'addition_time_embed_dim': None, 'attention_head_dim': 8, 'attention_type': 'default', 'center_input_sample': False, 'class_embeddings_concat': False, 'conv_in_kernel': 3, 'conv_out_kernel': 3, 'cross_attention_norm': None, 'downsample_padding': 1, 'dropout': 0.0, 'dual_cross_attention': False, 'encoder_hid_dim': None, 'encoder_hid_dim_type': None, 'flip_sin_to_cos': True, 'freq_shift': 0, 'mid_block_only_cross_attention': None, 'mid_block_scale_factor': 1, 'norm_eps': 1e-05, 'norm_num_groups': 32, 'num_attention_heads': None, 'num_class_embeds': None, 'only_cross_attention': False, 'resnet_out_scale_factor': 1.0, 'resnet_skip_time_act': False, 'resnet_time_scale_shift': 'default', 'reverse_transformer_layers_per_block': None, 'time_cond_proj_dim': None, 'time_embedding_act_fn': None, 'time_embedding_dim': None, 'time_embedding_type': 'positional', 'timestep_post_act': None, 'transformer_layers_per_block': 1, 'upcast_attention': False, 'use_linear_projection': False} were passed to CustomUNetWithEmbeddings, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_unet_model(resolution=image_size)\n",
    "from diffusers import UNet2DConditionModel\n",
    "model = CustomUNetWithEmbeddings.from_pretrained(model_path, subfolder=\"unet\")\n",
    "# # Load model checkpoint\n",
    "# try:\n",
    "#     # Try loading state dict directly\n",
    "#     state_dict = torch.load(os.path.join(model_path, \"unet\", \"diffusion_pytorch_model.bin\"), map_location=\"cpu\")\n",
    "#     model.load_state_dict(state_dict)\n",
    "# except:\n",
    "#     # Fallback to loading from checkpoint file\n",
    "#     checkpoint = torch.load(os.path.join(model_path, \"checkpoint.pt\"), map_location=\"cpu\")\n",
    "#     if \"model\" in checkpoint:\n",
    "#         model.load_state_dict(checkpoint[\"model\"])\n",
    "#     else:\n",
    "#         model.load_state_dict(checkpoint)\n",
    "\n",
    "# Move model to device and set to evaluation mode\n",
    "model.to(device)\n",
    "model.to(weight_dtype)\n",
    "model.eval()\n",
    "\n",
    "# Load VAE\n",
    "class DummyAccelerator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "dummy_accelerator = DummyAccelerator(device)\n",
    "vae = load_vae(vae_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "# Load scheduler\n",
    "scheduler = create_edm_scheduler(\n",
    "    sigma_min=sigma_min,\n",
    "    sigma_max=sigma_max,\n",
    "    sigma_data=sigma_data,\n",
    "    num_train_timesteps=1000,\n",
    "    prediction_type=\"sample\"\n",
    ")\n",
    "\n",
    "# Move scheduler sigmas to device\n",
    "scheduler.sigmas = scheduler.sigmas.to(device)\n",
    "\n",
    "# Load CLIP model (optional)\n",
    "clip_model = load_clip_model(clip_model_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "# Load classifier (optional)\n",
    "classifier = load_classifier(classifier_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_latent_sample(vae, images, weight_dtype=torch.float32):\n",
    "    \"\"\"Encode images to latent space using VAE\"\"\"\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(images).latent_dist.sample()\n",
    "    return latent\n",
    "\n",
    "def prepare_model_inputs(gt_images_latent, cond_images_latent, cell_line, label, dropout_prob=0.0, weight_dtype=torch.float32, encoder_hidden_states=None):\n",
    "    \"\"\"Prepare model inputs including latents and conditioning\"\"\"\n",
    "    # Combine protein and cell line for conditioning\n",
    "    batch_size = cond_images_latent.shape[0]\n",
    "    \n",
    "    # Create dropout mask for classifier-free guidance\n",
    "    dropout_mask = torch.rand(batch_size) > dropout_prob\n",
    "    \n",
    "    # Create full label tensor including cell line and label\n",
    "    total_label = torch.cat([cell_line, label], dim=1).to(weight_dtype)\n",
    "    \n",
    "    # Create a clean latent by combining ground truth and conditioning latents\n",
    "    clean_images = torch.cat([gt_images_latent, cond_images_latent], dim=1)\n",
    "    \n",
    "    return clean_images, total_label, encoder_hidden_states, dropout_mask\n",
    "\n",
    "def decode_latents(vae, latents, scaling_factor=4.0):\n",
    "    \"\"\"Decode latent samples to images using VAE\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Scale latents\n",
    "        latents = latents * 4 / vae.scaling_factor\n",
    "        \n",
    "        # Decode the latents to images\n",
    "        images = vae.decode(latents).sample\n",
    "        \n",
    "        # Normalize images to [0, 1] range\n",
    "        images = (images / 2 + 0.5).clamp(0, 1)\n",
    "        \n",
    "    return images\n",
    "\n",
    "def prepare_conditioning(clip_image=None, cell_line=None, label=None, batch_size=1, device=\"cuda\", weight_dtype=torch.float32):\n",
    "    \"\"\"Prepare conditioning inputs\"\"\"\n",
    "    # Process CLIP image if provided\n",
    "    if clip_image is not None:\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = clip_model(clip_image)\n",
    "    else:\n",
    "        # Create empty encoder hidden states\n",
    "        encoder_hidden_states = torch.zeros((batch_size, 196, 768), device=device, dtype=weight_dtype)\n",
    "    \n",
    "    # Set up cell line and label conditioning\n",
    "    if cell_line is None:\n",
    "        # Create a one-hot vector for cell line (assuming 40 cell lines)\n",
    "        cell_line = torch.zeros((batch_size, 40), device=device, dtype=weight_dtype)\n",
    "        cell_line[:, 0] = 1.0  # Set first cell line as default\n",
    "    \n",
    "    if label is None:\n",
    "        # Create a one-hot vector for label (assuming 13348 labels)\n",
    "        label = torch.zeros((batch_size, 13348), device=device, dtype=weight_dtype)\n",
    "        label[:, 0] = 1.0  # Set first label as default\n",
    "    \n",
    "    total_label = torch.cat([cell_line, label], dim=1)\n",
    "    \n",
    "    return encoder_hidden_states, total_label\n",
    "\n",
    "# def plot_images(images, row_title=None, **kwargs):\n",
    "#     \"\"\"Plot a grid of images\"\"\"\n",
    "#     if not isinstance(images, list):\n",
    "#         images = [images]\n",
    "    \n",
    "#     num_images = len(images)\n",
    "#     fig, axs = plt.subplots(1, num_images, figsize=(12, 12 // num_images))\n",
    "    \n",
    "#     if num_images == 1:\n",
    "#         axs = [axs]\n",
    "    \n",
    "#     for i, img in enumerate(images):\n",
    "#         if isinstance(img, torch.Tensor):\n",
    "#             img = img.detach().cpu().numpy()\n",
    "        \n",
    "#         # Handle different shapes and channel configurations\n",
    "#         if img.ndim == 4 and img.shape[0] == 1:  # [1, C, H, W]\n",
    "#             img = img[0]\n",
    "        \n",
    "#         if img.shape[0] == 3 or img.shape[0] == 1:  # [C, H, W]\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "        \n",
    "#         if img.shape[-1] == 1:  # Single channel\n",
    "#             img = img.squeeze(-1)\n",
    "#             axs[i].imshow(img, cmap='gray')\n",
    "#         else:  # RGB\n",
    "#             axs[i].imshow(img)\n",
    "        \n",
    "#         axs[i].set_xticks([])\n",
    "#         axs[i].set_yticks([])\n",
    "    \n",
    "#     if row_title is not None:\n",
    "#         fig.suptitle(row_title)\n",
    "#     #save the figure\n",
    "#     plt.savefig(f\"{row_title}.png\")\n",
    "#     plt.tight_layout()\n",
    "#     #plt.show()\n",
    "    \n",
    "def plot_images(images, row_title=None, **kwargs):\n",
    "    \"\"\"Plot a grid of images and save to file without displaying\"\"\"\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "    \n",
    "    num_images = len(images)\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(12, 12 // num_images))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().cpu().numpy()\n",
    "        \n",
    "        # Handle different shapes and channel configurations\n",
    "        if img.ndim == 4 and img.shape[0] == 1:  # [1, C, H, W]\n",
    "            img = img[0]\n",
    "        \n",
    "        if img.shape[0] == 3 or img.shape[0] == 1:  # [C, H, W]\n",
    "            img = img.transpose(1, 2, 0)\n",
    "        \n",
    "        if img.shape[-1] == 1:  # Single channel\n",
    "            img = img.squeeze(-1)\n",
    "            axs[i].imshow(img, cmap='gray')\n",
    "        else:  # RGB\n",
    "            axs[i].imshow(img)\n",
    "        \n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "    \n",
    "    if row_title is not None:\n",
    "        fig.suptitle(row_title)\n",
    "    \n",
    "    # Save the figure with appropriate filename\n",
    "    filename = f\"{row_title}.png\" if row_title else \"plot.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # Close the figure to prevent display and free memory\n",
    "    \n",
    "    \n",
    "def save_image(image, output_filename=\"output.png\", **kwargs):\n",
    "    \"\"\"Save a single image to PNG file without using matplotlib\"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Handle torch tensor\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.detach().cpu().numpy()\n",
    "    \n",
    "    # Handle different shapes and channel configurations\n",
    "    if image.ndim == 4 and image.shape[0] == 1:  # [1, C, H, W]\n",
    "        image = image[0]\n",
    "    \n",
    "    if image.shape[0] == 3 or image.shape[0] == 1:  # [C, H, W]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "    \n",
    "    # Handle single channel images\n",
    "    if image.shape[-1] == 1:  # Single channel\n",
    "        image = image.squeeze(-1)\n",
    "        # For grayscale, just use L mode\n",
    "        pil_mode = 'L'\n",
    "    else:  # RGB\n",
    "        pil_mode = 'RGB'\n",
    "    \n",
    "    # Ensure values are in valid range for PIL\n",
    "    if image.max() <= 1.0:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    else:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Create and save the PIL image\n",
    "    if not output_filename.endswith('.png'):\n",
    "        output_filename += '.png'\n",
    "        \n",
    "    pil_img = Image.fromarray(image, mode=pil_mode)\n",
    "    pil_img.save(output_filename)\n",
    "    \n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test dataset to get real conditioning images\n",
    "try:\n",
    "    test_dataset = FullFieldDataset(\n",
    "        data_root='/home/pc/Documents/twisted_diffusion_helper_model/test_images',\n",
    "        label_dict='/home/pc/Documents/twisted_diffusion_helper_model/antibody_map.pkl',\n",
    "        annotation_dict='/home/pc/Documents/twisted_diffusion_helper_model/annotation_map.pkl'\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "    )\n",
    "    # Get a batch of test data\n",
    "    batch = next(iter(test_dataloader))\n",
    "    \n",
    "    # Show conditioning image\n",
    "    cond_images = batch[\"cond_image\"].to(weight_dtype).to(device)\n",
    "    clip_images = batch[\"clip_image\"].to(weight_dtype).to(device)\n",
    "    gt_images = batch[\"gt_image\"].to(weight_dtype).to(device)\n",
    "    # Encode conditioning image to latent space\n",
    "    with torch.no_grad():\n",
    "        #cond_images_latent = prepare_latent_sample(vae, cond_images.repeat(1, 3, 1, 1), weight_dtype)\n",
    "        encoder_hidden_states = clip_model(clip_images)\n",
    "        \n",
    "    # Prepare cell_line and label conditioning\n",
    "    cell_line = batch[\"cell_line\"].to(device).long()\n",
    "    protein_label = batch[\"label\"].to(device).long()\n",
    "    #one hot encoding\n",
    "    #cell_line = torch.nn.functional.one_hot(cell_line, num_classes=40)\n",
    "    #label = torch.nn.functional.one_hot(label, num_classes=13348)\n",
    "    \n",
    "    #total_label = torch.cat([cell_line, label], dim=1)\n",
    "    \n",
    "    # Display conditioning image\n",
    "    for i in range(batch_size):\n",
    "        plot_images(cond_images[i].cpu().float()*0.5+0.5, row_title=f\"generated_images/Testset_Conditioning Image_{i}\")\n",
    "        plot_images(gt_images[i].cpu().float()*0.5+0.5, row_title=f\"generated_images/Testset_Ground Truth Image_{i}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load test dataset: {e}\")\n",
    "    print(\"Generating without real conditioning images...\")\n",
    "    \n",
    "    # Create random conditioning\n",
    "    batch_size = 4\n",
    "    \n",
    "    # Random latent for conditioning (using 8 channels for conditioning)\n",
    "    cond_images_latent = torch.randn(\n",
    "        (batch_size, 8, image_size, image_size),\n",
    "        device=device, \n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    \n",
    "    # Random CLIP embeddings\n",
    "    encoder_hidden_states = torch.randn(\n",
    "        (batch_size, 196, 768),\n",
    "        device=device,\n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    \n",
    "    # Create a one-hot vector for cell line (assuming 40 cell lines)\n",
    "    cell_line = torch.zeros((batch_size, 40), device=device, dtype=weight_dtype)\n",
    "    cell_line[:, 0] = 1.0  # Set first cell line\n",
    "    \n",
    "    # Create a one-hot vector for protein label (assuming 13348 labels)\n",
    "    label = torch.zeros((batch_size, 13348), device=device, dtype=weight_dtype)\n",
    "    label[:, 0] = 1.0  # Set first label\n",
    "    \n",
    "    total_label = torch.cat([cell_line, label], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_edm(\n",
    "    model,\n",
    "    scheduler,\n",
    "    batch_size=1,\n",
    "    image_size=32,\n",
    "    num_inference_steps=50,\n",
    "    condition_latent=None,  # Optional conditioning latent\n",
    "    encoder_hidden_states=None,  # CLIP hidden states\n",
    "    class_labels=None,  # Class labels for conditioning\n",
    "    \n",
    "    protein_labels=None,\n",
    "    cell_line_labels=None,\n",
    "    \n",
    "    guidance_scale=1.0,  # Scale for classifier-free guidance\n",
    "    generator=None,\n",
    "    output_type=\"latent\",  # \"latent\" or \"pt\"\n",
    "):\n",
    "    \"\"\"Sample from the diffusion model using EDM sampling\"\"\"\n",
    "    # Initialize with random noise for the ground truth part\n",
    "    if condition_latent is not None:\n",
    "        latent_channels = 32 - condition_latent.shape[1]  # Get channels for gt part\n",
    "    else:\n",
    "        latent_channels = 32\n",
    "        condition_latent = torch.zeros((batch_size, 0, image_size, image_size), device=device, dtype=weight_dtype)\n",
    "    \n",
    "    # Create random noise for the ground truth part\n",
    "    gt_noise = torch.randn(\n",
    "        (batch_size, latent_channels, image_size, image_size),\n",
    "        generator=generator,\n",
    "        device=device,\n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    \n",
    "    # Initialize with random noise * sigma_max for the ground truth part\n",
    "    latents = gt_noise * scheduler.sigmas[0].to(device)\n",
    "    \n",
    "    # Set up classifier-free guidance conditioning (if needed)\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "    \n",
    "    # Set up progress bar\n",
    "    progress_bar = tqdm(range(num_inference_steps))\n",
    "    progress_bar.set_description(\"Sampling\")\n",
    "    \n",
    "    # Set timesteps for sampling\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps = scheduler.timesteps\n",
    "    \n",
    "    # Sampling loop\n",
    "    for i, t in enumerate(progress_bar):\n",
    "        # Get sigma for this step\n",
    "        sigma = scheduler.sigmas[i]\n",
    "        sigma_next = scheduler.sigmas[i + 1] if i < len(scheduler.sigmas) - 1 else torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Expand sigma for broadcasting\n",
    "        sigma_expanded = sigma.expand(batch_size).to(device)\n",
    "        sigma_view = sigma_expanded.view(-1, 1, 1, 1).double()\n",
    "        \n",
    "        \n",
    "        latents = latents.double()\n",
    "        \n",
    "        # Combine latents with condition latent\n",
    "        #combined_latent = torch.cat([latents, condition_latent], dim=1)\n",
    "        combined_latent = latents\n",
    "        # Prepare input with noise according to EDM formulation\n",
    "        model_input, timestep_input = edm_clean_image_to_model_input(combined_latent, sigma_view)\n",
    "        timestep_input = timestep_input.squeeze()\n",
    "        \n",
    "        # For classifier-free guidance, we need to do two forward passes:\n",
    "        # one with the conditioning and one without\n",
    "        if do_classifier_free_guidance:\n",
    "            # Unconditional forward pass\n",
    "            model_output_uncond = model(\n",
    "                model_input,\n",
    "                timestep_input,\n",
    "                class_labels=None,  # No class conditioning\n",
    "                encoder_hidden_states=None,  # No CLIP conditioning\n",
    "            ).sample\n",
    "            \n",
    "            # Conditional forward pass\n",
    "            model_output_cond = model(\n",
    "                model_input,\n",
    "                timestep_input,\n",
    "                class_labels=class_labels,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "            ).sample\n",
    "            \n",
    "            # Combine outputs with guidance scale\n",
    "            model_output = model_output_uncond + guidance_scale * (model_output_cond - model_output_uncond)\n",
    "        else:\n",
    "            # Regular conditional forward pass\n",
    "            model.to(weight_dtype)\n",
    "            model_input = model_input.to(weight_dtype)\n",
    "            timestep_input = timestep_input.to(weight_dtype)\n",
    "            #class_labels = class_labels.to(weight_dtype)\n",
    "            protein_labels = protein_labels\n",
    "            cell_line_labels = cell_line_labels\n",
    "            encoder_hidden_states = encoder_hidden_states.to(weight_dtype)\n",
    "            model_output = model(\n",
    "                model_input,\n",
    "                timestep_input,\n",
    "                protein_labels = protein_labels,\n",
    "                cell_line_labels = cell_line_labels,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "            ).sample\n",
    "        \n",
    "        # Convert model output to denoised latent (x0 prediction)\n",
    "        pred_x0 = edm_model_output_to_x_0_hat(combined_latent, sigma_view, model_output.double())\n",
    "        \n",
    "        # Extract only the ground truth part (not the conditioning part)\n",
    "        #pred_x0 = pred_x0[:, :latent_channels, :, :]\n",
    "\n",
    "        # Step using the scheduler\n",
    "        # We implement a simplified Euler step for the EDM sampler\n",
    "        denoised = pred_x0\n",
    "        step_sigma = sigma - sigma_next\n",
    "        step_size = step_sigma / sigma\n",
    "        \n",
    "        # Calculate D(xt) (denoised - noisy) / sigma as the \"direction\"\n",
    "        direction = (denoised - latents) / sigma_view\n",
    "        \n",
    "        # Euler step\n",
    "        latents = latents + step_size.item() * sigma_view * direction\n",
    "        \n",
    "        # Add noise if not the last step\n",
    "        # if i < num_inference_steps - 1:\n",
    "        #     noise = torch.randn_like(latents)\n",
    "        #     latents = latents + ((sigma_next**2 - sigma_next**2) ** 0.5) * noise\n",
    "    \n",
    "    # Return the final latents or decode to images based on output_type\n",
    "    if output_type == \"latent\":\n",
    "        return latents\n",
    "    elif output_type == \"pt\":\n",
    "        # Return PyTorch tensor\n",
    "        return latents\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output_type: {output_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 1000/1000 [00:18<00:00, 53.46it/s]\n",
      "/tmp/ipykernel_1392503/385784162.py:27: FutureWarning: Accessing config attribute `scaling_factor` directly via 'AutoencoderKL' object attribute is deprecated. Please access 'scaling_factor' over 'AutoencoderKL's config object instead, e.g. 'unet.config.scaling_factor'.\n",
      "  latents = latents * 4 / vae.scaling_factor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16 images\n"
     ]
    }
   ],
   "source": [
    "# Generate samples\n",
    "weight_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "num_inference_steps=1000\n",
    "with torch.no_grad():\n",
    "    # Set random seed for reproducibility\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    # Sample from the model\n",
    "    generated_latents = sample_edm(\n",
    "        model=model,\n",
    "        scheduler=scheduler,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        condition_latent=None,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        class_labels=None,\n",
    "        protein_labels = protein_label,\n",
    "        cell_line_labels = cell_line,\n",
    "        guidance_scale=0,\n",
    "        generator=generator,\n",
    "        output_type=\"latent\",\n",
    "    )\n",
    "    \n",
    "    # Decode the latents to images\n",
    "    vae.to(torch.float32)\n",
    "    generated_latents = generated_latents.to(torch.float32)\n",
    "    generated_images_gt = decode_latents(vae, generated_latents[:,:16,:,:])\n",
    "    generated_images_cond = decode_latents(vae, generated_latents[:,16:,:,:])\n",
    "    \n",
    "print(f\"Generated {len(generated_images_gt)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated images\n",
    "for i, img in enumerate(generated_images_gt):\n",
    "    plot_images(generated_images_gt[i].cpu(), row_title=f\"generated_images/Generated Ground Truth Image_{i}\")\n",
    "    plot_images(generated_images_cond[i].cpu(), row_title=f\"generated_images/Generated Conditioning Image_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_edm(\n",
    "    model,\n",
    "    scheduler,\n",
    "    batch_size=1,\n",
    "    image_size=32,\n",
    "    num_inference_steps=50,\n",
    "    condition_latent=None,  # Optional conditioning latent\n",
    "    encoder_hidden_states=None,  # CLIP hidden states\n",
    "    class_labels=None,  # Class labels for conditioning\n",
    "    \n",
    "    protein_labels=None,\n",
    "    cell_line_labels=None,\n",
    "    \n",
    "    guidance_scale=1.0,  # Scale for classifier-free guidance\n",
    "    generator=None,\n",
    "    output_type=\"latent\",  # \"latent\" or \"pt\"\n",
    "    \n",
    "    # Add new parameters from EDMEulerScheduler.step method\n",
    "    s_churn=0.0,\n",
    "    s_tmin=0.0,\n",
    "    s_tmax=float(\"inf\"),\n",
    "    s_noise=1.0,\n",
    "):\n",
    "    \"\"\"Sample from the diffusion model using EDM sampling\"\"\"\n",
    "    # Initialize with random noise for the ground truth part\n",
    "    if condition_latent is not None:\n",
    "        latent_channels = 32 - condition_latent.shape[1]  # Get channels for gt part\n",
    "    else:\n",
    "        latent_channels = 32\n",
    "        condition_latent = torch.zeros((batch_size, 0, image_size, image_size), device=device, dtype=weight_dtype)\n",
    "    \n",
    "    # Create random noise for the ground truth part\n",
    "    gt_noise = torch.randn(\n",
    "        (batch_size, latent_channels, image_size, image_size),\n",
    "        generator=generator,\n",
    "        device=device,\n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    \n",
    "    # Initialize with random noise * sigma_max for the ground truth part\n",
    "    latents = gt_noise * scheduler.sigmas[0].to(device)\n",
    "    \n",
    "    # Set up classifier-free guidance conditioning (if needed)\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "    \n",
    "    # Set up progress bar\n",
    "    progress_bar = tqdm(range(num_inference_steps))\n",
    "    progress_bar.set_description(\"Sampling\")\n",
    "    \n",
    "    # Set timesteps for sampling\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps = scheduler.timesteps\n",
    "    \n",
    "    # Sampling loop\n",
    "    for i, t in enumerate(progress_bar):\n",
    "        # Get sigma for this step\n",
    "        sigma = scheduler.sigmas[i]\n",
    "        sigma_next = scheduler.sigmas[i + 1] if i < len(scheduler.sigmas) - 1 else torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Expand sigma for broadcasting\n",
    "        sigma_expanded = sigma.expand(batch_size).to(device)\n",
    "        sigma_view = sigma_expanded.view(-1, 1, 1, 1).double()\n",
    "        \n",
    "        # Calculate gamma for stochastic sampling (from the step method)\n",
    "        gamma = min(s_churn / (len(scheduler.sigmas) - 1), 2**0.5 - 1) if s_tmin <= sigma <= s_tmax else 0.0\n",
    "        sigma_hat = sigma * (gamma + 1)\n",
    "        sigma_hat_view = sigma_hat.view(-1, 1, 1, 1).double().to(device)\n",
    "        \n",
    "        # Add noise if gamma > 0 (s_churn is active) - implements stochastic sampling\n",
    "        if gamma > 0:\n",
    "            noise = torch.randn((batch_size, latent_channels, image_size, image_size), generator=generator, device=device, dtype=latents.dtype)\n",
    "            eps = noise * s_noise\n",
    "            latents = latents + eps * (sigma_hat**2 - sigma**2) ** 0.5\n",
    "        \n",
    "        latents = latents.double()\n",
    "        \n",
    "        # Combine latents with condition latent\n",
    "        combined_latent = latents\n",
    "        \n",
    "        # Prepare input with noise according to EDM formulation\n",
    "        model_input, timestep_input = edm_clean_image_to_model_input(combined_latent, sigma_hat_view)\n",
    "        timestep_input = timestep_input.squeeze()\n",
    "        \n",
    "        # For classifier-free guidance, we need to do two forward passes:\n",
    "        # one with the conditioning and one without\n",
    "\n",
    "        # Regular conditional forward pass\n",
    "        model.to(weight_dtype)\n",
    "        model_input = model_input.to(weight_dtype)\n",
    "        timestep_input = timestep_input.to(weight_dtype)\n",
    "        \n",
    "        if encoder_hidden_states is not None:\n",
    "            encoder_hidden_states = encoder_hidden_states.to(weight_dtype)\n",
    "            \n",
    "        model_output = model(\n",
    "            model_input,\n",
    "            timestep_input,\n",
    "            protein_labels=protein_labels,\n",
    "            cell_line_labels=cell_line_labels,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        ).sample\n",
    "    \n",
    "        # Convert model output to denoised latent (x0 prediction)\n",
    "        pred_x0 = edm_model_output_to_x_0_hat(combined_latent, sigma_hat_view, model_output.double())\n",
    "        \n",
    "        \n",
    "        d_cur = (latents - pred_x0) / sigma_hat_view\n",
    "        latents = latents + (sigma_next - sigma_hat) * d_cur\n",
    "        \n",
    "        # # Convert to an ODE derivative (matches formula in EDMEulerScheduler)\n",
    "        # derivative = (latents - pred_x0) / sigma_hat_view\n",
    "        \n",
    "        # # Calculate step size\n",
    "        # dt = sigma_next - sigma_hat\n",
    "        \n",
    "        # # Euler step\n",
    "        # latents = latents + derivative * dt.item()\n",
    "    \n",
    "    # Return the final latents or decode to images based on output_type\n",
    "    if output_type == \"latent\":\n",
    "        return latents\n",
    "    elif output_type == \"pt\":\n",
    "        # Return PyTorch tensor\n",
    "        return latents\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output_type: {output_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/174 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 1000/1000 [01:12<00:00, 13.70it/s]\n",
      "/tmp/ipykernel_1522208/4242947897.py:27: FutureWarning: Accessing config attribute `scaling_factor` directly via 'AutoencoderKL' object attribute is deprecated. Please access 'scaling_factor' over 'AutoencoderKL's config object instead, e.g. 'unet.config.scaling_factor'.\n",
      "  latents = latents * 4 / vae.scaling_factor\n",
      "  0%|          | 0/174 [01:16<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = FullFieldDataset(\n",
    "        data_root='/home/pc/Documents/twisted_diffusion_helper_model/test_images',\n",
    "        label_dict='/home/pc/Documents/twisted_diffusion_helper_model/antibody_map.pkl',\n",
    "        annotation_dict='/home/pc/Documents/twisted_diffusion_helper_model/annotation_map.pkl'\n",
    "    )\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n",
    "# Get a batch of test data\n",
    "#batch = next(iter(test_dataloader))\n",
    "from tqdm import tqdm\n",
    "    # Generate samples\n",
    "weight_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "clip_model.to(weight_dtype)\n",
    "count = 0\n",
    "for batch in tqdm(test_dataloader):\n",
    "\n",
    "    # Show conditioning image\n",
    "    cond_images = batch[\"cond_image\"].to(weight_dtype).to(device)\n",
    "    clip_images = batch[\"clip_image\"].to(weight_dtype).to(device)\n",
    "    gt_images = batch[\"gt_image\"].to(weight_dtype).to(device)\n",
    "    # Encode conditioning image to latent space\n",
    "    with torch.no_grad():\n",
    "        #cond_images_latent = prepare_latent_sample(vae, cond_images.repeat(1, 3, 1, 1), weight_dtype)\n",
    "        encoder_hidden_states = clip_model(clip_images)\n",
    "        \n",
    "    # Prepare cell_line and label conditioning\n",
    "    cell_line = batch[\"cell_line\"].to(device).long()\n",
    "    protein_label = batch[\"label\"].to(device).long()\n",
    "    #one hot encoding\n",
    "    #cell_line = torch.nn.functional.one_hot(cell_line, num_classes=40)\n",
    "    #label = torch.nn.functional.one_hot(label, num_classes=13348)\n",
    "\n",
    "    #total_label = torch.cat([cell_line, label], dim=1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    num_inference_steps=1000\n",
    "    with torch.no_grad():\n",
    "        # Set random seed for reproducibility\n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "        \n",
    "        # Sample from the model\n",
    "        generated_latents = sample_edm(\n",
    "            model=model,\n",
    "            scheduler=scheduler,\n",
    "            batch_size=batch_size,\n",
    "            image_size=image_size,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            condition_latent=None,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            class_labels=None,\n",
    "            protein_labels = protein_label,\n",
    "            cell_line_labels = cell_line,\n",
    "            guidance_scale=0,\n",
    "            generator=generator,\n",
    "            output_type=\"latent\",\n",
    "            s_churn = 0\n",
    "        )\n",
    "        \n",
    "        # Decode the latents to images\n",
    "        vae.to(torch.float32)\n",
    "        generated_latents = generated_latents.to(torch.float32)\n",
    "        generated_images_gt = decode_latents(vae, generated_latents[:,:16,:,:])\n",
    "        generated_images_cond = decode_latents(vae, generated_latents[:,16:,:,:])\n",
    "    \n",
    "        # Display conditioning image\n",
    "    for i in range(batch_size):\n",
    "        save_image(cond_images[i].cpu().float()*0.5+0.5, output_filename=f\"generated_images/Testset_Conditioning Image_{count}\")\n",
    "        save_image(generated_images_gt[i].cpu(), output_filename=f\"generated_images/Generated Ground Truth Image_{count}\")\n",
    "        save_image(gt_images[i].cpu().float()*0.5+0.5, output_filename=f\"generated_images/Testset_Ground Truth Image_{count}\")\n",
    "        save_image(generated_images_cond[i].cpu(), output_filename=f\"generated_images/Generated Conditioning Image_{count}\")\n",
    "        count += 1\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cond_images[i].cpu().float()*0.5+0.5).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation with Different Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_parameters(\n",
    "    num_images=4,\n",
    "    guidance_scales=[1.0, 3.0, 5.0, 7.0],\n",
    "    num_inference_steps=50,\n",
    "    random_seed=42,\n",
    "):\n",
    "    \"\"\"Generate multiple images with different parameters\"\"\"\n",
    "    all_images = []\n",
    "    \n",
    "    # Setup conditioning if not already defined\n",
    "    if 'cond_images_latent' not in globals():\n",
    "        # Create random conditioning\n",
    "        cond_images_latent = torch.randn(\n",
    "            (num_images, 8, image_size, image_size),\n",
    "            device=device, \n",
    "            dtype=weight_dtype\n",
    "        )\n",
    "        \n",
    "        # Random CLIP embeddings\n",
    "        encoder_hidden_states = torch.randn(\n",
    "            (num_images, 196, 768),\n",
    "            device=device,\n",
    "            dtype=weight_dtype\n",
    "        )\n",
    "        \n",
    "        # Create a one-hot vector for cell line (assuming 40 cell lines)\n",
    "        cell_line = torch.zeros((num_images, 40), device=device, dtype=weight_dtype)\n",
    "        cell_line[:, 0] = 1.0  # Set first cell line\n",
    "        \n",
    "        # Create a one-hot vector for protein label (assuming 13348 labels)\n",
    "        label = torch.zeros((num_images, 13348), device=device, dtype=weight_dtype)\n",
    "        label[:, 0] = 1.0  # Set first label\n",
    "        \n",
    "        total_label = torch.cat([cell_line, label], dim=1)\n",
    "    else:\n",
    "        # Use globally defined conditioning\n",
    "        pass\n",
    "    \n",
    "    # Generate samples with different guidance scales\n",
    "    for guidance_scale in guidance_scales:\n",
    "        with torch.no_grad():\n",
    "            # Set random seed for reproducibility\n",
    "            generator = torch.Generator(device=device).manual_seed(random_seed)\n",
    "            \n",
    "            # Sample from the model\n",
    "            latents = sample_edm(\n",
    "                model=model,\n",
    "                scheduler=scheduler,\n",
    "                batch_size=num_images,\n",
    "                image_size=image_size,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                condition_latent=cond_images_latent,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                class_labels=total_label,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                output_type=\"latent\",\n",
    "            )\n",
    "            \n",
    "            # Decode the latents to images\n",
    "            images = decode_latents(vae, latents)\n",
    "            \n",
    "            all_images.append({\"guidance_scale\": guidance_scale, \"images\": images})\n",
    "    \n",
    "    # Plot images with different guidance scales\n",
    "    num_rows = len(all_images)\n",
    "    num_cols = num_images\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))\n",
    "    \n",
    "    for row_idx, result in enumerate(all_images):\n",
    "        guidance_scale = result[\"guidance_scale\"]\n",
    "        images = result[\"images\"]\n",
    "        \n",
    "        for col_idx, img in enumerate(images):\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img = img.detach().cpu().numpy()\n",
    "            \n",
    "            # Handle different shapes and channel configurations\n",
    "            if img.ndim == 4 and img.shape[0] == 1:  # [1, C, H, W]\n",
    "                img = img[0]\n",
    "            \n",
    "            if img.shape[0] == 3 or img.shape[0] == 1:  # [C, H, W]\n",
    "                img = img.transpose(1, 2, 0)\n",
    "            \n",
    "            if img.shape[-1] == 1:  # Single channel\n",
    "                img = img.squeeze(-1)\n",
    "                axs[row_idx, col_idx].imshow(img, cmap='gray')\n",
    "            else:  # RGB\n",
    "                axs[row_idx, col_idx].imshow(img)\n",
    "            \n",
    "            axs[row_idx, col_idx].set_xticks([])\n",
    "            axs[row_idx, col_idx].set_yticks([])\n",
    "            \n",
    "            if col_idx == 0:\n",
    "                axs[row_idx, col_idx].set_ylabel(f\"CFG: {guidance_scale}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "class_labels should be provided when num_class_embeds > 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate images with different guidance scales\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_with_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Faster sampling for demonstration\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m, in \u001b[0;36mgenerate_with_parameters\u001b[0;34m(num_images, guidance_scales, num_inference_steps, random_seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mmanual_seed(random_seed)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Sample from the model\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[43msample_edm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition_latent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_images_latent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Decode the latents to images\u001b[39;00m\n\u001b[1;32m     61\u001b[0m images \u001b[38;5;241m=\u001b[39m decode_latents(vae, latents)\n",
      "Cell \u001b[0;32mIn[7], line 98\u001b[0m, in \u001b[0;36msample_edm\u001b[0;34m(model, scheduler, batch_size, image_size, num_inference_steps, condition_latent, encoder_hidden_states, class_labels, protein_labels, cell_line_labels, guidance_scale, generator, output_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m     cell_line_labels \u001b[38;5;241m=\u001b[39m cell_line_labels\n\u001b[1;32m     97\u001b[0m     encoder_hidden_states \u001b[38;5;241m=\u001b[39m encoder_hidden_states\u001b[38;5;241m.\u001b[39mto(weight_dtype)\n\u001b[0;32m---> 98\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprotein_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprotein_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcell_line_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell_line_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Convert model output to denoised latent (x0 prediction)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m pred_x0 \u001b[38;5;241m=\u001b[39m edm_model_output_to_x_0_hat(combined_latent, sigma_view, model_output\u001b[38;5;241m.\u001b[39mdouble())\n",
      "File \u001b[0;32m~/anaconda3/envs/image_tds/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/image_tds/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/twisted_diffusion/models/unet.py:77\u001b[0m, in \u001b[0;36mCustomUNetWithEmbeddings.forward\u001b[0;34m(self, sample, timestep, protein_labels, cell_line_labels, encoder_hidden_states, class_labels, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m final_class_labels \u001b[38;5;241m=\u001b[39m computed_class_labels \u001b[38;5;28;01mif\u001b[39;00m computed_class_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m class_labels\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# --- Call the original UNet2DConditionModel forward ---\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Pass the computed embedding as 'class_labels'\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_class_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the computed embedding\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass through any other args\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/image_tds/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py:1143\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_time_embed(sample\u001b[38;5;241m=\u001b[39msample, timestep\u001b[38;5;241m=\u001b[39mtimestep)\n\u001b[1;32m   1141\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding(t_emb, timestep_cond)\n\u001b[0;32m-> 1143\u001b[0m class_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_emb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclass_embeddings_concat:\n",
      "File \u001b[0;32m~/anaconda3/envs/image_tds/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py:938\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_class_embed\u001b[0;34m(self, sample, class_labels)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m class_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels should be provided when num_class_embeds > 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclass_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    941\u001b[0m         class_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_proj(class_labels)\n",
      "\u001b[0;31mValueError\u001b[0m: class_labels should be provided when num_class_embeds > 0"
     ]
    }
   ],
   "source": [
    "# Generate images with different guidance scales\n",
    "generate_with_parameters(\n",
    "    num_images=4,\n",
    "    guidance_scales=[1.0, 3.0, 5.0, 7.0],\n",
    "    num_inference_steps=30,  # Faster sampling for demonstration\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, output_dir=\"generated_images\", prefix=\"sample\", format=\"png\"):\n",
    "    \"\"\"Save generated images to disk\"\"\"\n",
    "    from PIL import Image\n",
    "    import os\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = (img * 255).clamp(0, 255).cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "        # Convert from [C, H, W] to [H, W, C]\n",
    "        if img.shape[0] == 3 or img.shape[0] == 1:\n",
    "            img = img.transpose(1, 2, 0)\n",
    "        \n",
    "        # Remove single-dimension channel for grayscale\n",
    "        if img.shape[-1] == 1:\n",
    "            img = img.squeeze(-1)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        pil_img = Image.fromarray(img)\n",
    "        \n",
    "        # Save image\n",
    "        file_path = os.path.join(output_dir, f\"{prefix}_{i:04d}.{format}\")\n",
    "        pil_img.save(file_path)\n",
    "        print(f\"Saved {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated images\n",
    "save_images(generated_images, output_dir=\"generated_images\", prefix=\"edm_sample\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_tds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
