{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twisted Diffusion Model Sampler\n",
    "\n",
    "This notebook provides a sampler for the trained diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mli89/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import project modules\n",
    "from models.unet import create_unet_model, load_vae, load_classifier, load_clip_model, CustomUNetWithEmbeddings\n",
    "from models.dit import create_dit_model, DiTTransformer2DModelWithCrossAttention\n",
    "from schedulers.edm_scheduler import create_edm_scheduler\n",
    "from utils.edm_utils import edm_clean_image_to_model_input, edm_model_output_to_x_0_hat\n",
    "from config.default_config import EDM_CONFIG\n",
    "from models.clip_image_encoder import OpenCLIPVisionEncoder\n",
    "from data.dataset import FullFieldDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up device and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set precision (you can adjust this based on your hardware)\n",
    "weight_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# # Define paths to models and checkpoints\n",
    "model_path = \"/scratch/groups/emmalu/marvinli/twisted_diffusion/sunh/two_labels_latent_diffusion_edm_silu_cross_attention/checkpoint-182000\"  # Update this to your model checkpoint path\n",
    "vae_path = \"/scratch/groups/emmalu/marvinli/twisted_diffusion/stable-diffusion-3.5-large-turbo/vae\"\n",
    "classifier_path = \"/scratch/groups/emmalu/marvinli/twisted_diffusion/checkpoints_classifier/model_epoch_7.pth\"\n",
    "clip_model_path = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "\n",
    "\n",
    "# Define paths to models and checkpoints\n",
    "# model_path = \"/home/pc/Documents/twisted_diffusion/two_labels_latent_diffusion_edm_silu_less_cross_attn/checkpoint-200000\"  # Update this to your model checkpoint path\n",
    "# vae_path = \"/home/pc/Documents/twisted_diffusion_helper_model/vae\"\n",
    "# classifier_path = \"/home/pc/Documents/twisted_diffusion_helper_model/checkpoints_classifier/model_epoch_7.pth\"\n",
    "# clip_model_path = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "\n",
    "# Set EDM parameters\n",
    "sigma_min = EDM_CONFIG[\"SIGMA_MIN\"]\n",
    "sigma_max = EDM_CONFIG[\"SIGMA_MAX\"]\n",
    "sigma_data = EDM_CONFIG[\"SIGMA_DATA\"]\n",
    "rho = EDM_CONFIG[\"RHO\"]\n",
    "\n",
    "# Set sampling parameters\n",
    "num_inference_steps = 100\n",
    "guidance_scale = 7.5  # Higher values increase adherence to the conditioning\n",
    "batch_size = 64\n",
    "image_size = 32  # Size of the generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "# model = create_unet_model(resolution=image_size)\n",
    "# from diffusers import UNet2DConditionModel\n",
    "# model = CustomUNetWithEmbeddings.from_pretrained(model_path, subfolder=\"unet\")\n",
    "\n",
    "\n",
    "#model = create_dit_model(config=None, resolution=32)\n",
    "model = DiTTransformer2DModelWithCrossAttention.from_pretrained(model_path, subfolder=\"unet\")\n",
    "\n",
    "# # Load model checkpoint\n",
    "# try:\n",
    "#     # Try loading state dict directly\n",
    "#     state_dict = torch.load(os.path.join(model_path, \"unet\", \"diffusion_pytorch_model.bin\"), map_location=\"cpu\")\n",
    "#     model.load_state_dict(state_dict)\n",
    "# except:\n",
    "#     # Fallback to loading from checkpoint file\n",
    "#     checkpoint = torch.load(os.path.join(model_path, \"checkpoint.pt\"), map_location=\"cpu\")\n",
    "#     if \"model\" in checkpoint:\n",
    "#         model.load_state_dict(checkpoint[\"model\"])\n",
    "#     else:\n",
    "#         model.load_state_dict(checkpoint)\n",
    "\n",
    "# Move model to device and set to evaluation mode\n",
    "model.to(device)\n",
    "model.to(weight_dtype)\n",
    "model.eval()\n",
    "\n",
    "# Load VAE\n",
    "class DummyAccelerator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "dummy_accelerator = DummyAccelerator(device)\n",
    "vae = load_vae(vae_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "# Load scheduler\n",
    "scheduler = create_edm_scheduler(\n",
    "    sigma_min=sigma_min,\n",
    "    sigma_max=sigma_max,\n",
    "    sigma_data=sigma_data,\n",
    "    num_train_timesteps=1000,\n",
    "    prediction_type=\"sample\"\n",
    ")\n",
    "\n",
    "# Move scheduler sigmas to device\n",
    "scheduler.sigmas = scheduler.sigmas.to(device)\n",
    "\n",
    "# Load CLIP model (optional)\n",
    "clip_model = load_clip_model(clip_model_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "# Load classifier (optional)\n",
    "classifier = load_classifier(classifier_path, dummy_accelerator, weight_dtype)\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_latent_sample(vae, images, weight_dtype=torch.float32):\n",
    "    \"\"\"Encode images to latent space using VAE\"\"\"\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(images).latent_dist.sample()\n",
    "    return latent\n",
    "\n",
    "def prepare_model_inputs(gt_images_latent, cond_images_latent, cell_line, label, dropout_prob=0.0, weight_dtype=torch.float32, encoder_hidden_states=None):\n",
    "    \"\"\"Prepare model inputs including latents and conditioning\"\"\"\n",
    "    # Combine protein and cell line for conditioning\n",
    "    batch_size = cond_images_latent.shape[0]\n",
    "    \n",
    "    # Create dropout mask for classifier-free guidance\n",
    "    dropout_mask = torch.rand(batch_size) > dropout_prob\n",
    "    \n",
    "    # Create full label tensor including cell line and label\n",
    "    total_label = torch.cat([cell_line, label], dim=1).to(weight_dtype)\n",
    "    \n",
    "    # Create a clean latent by combining ground truth and conditioning latents\n",
    "    clean_images = torch.cat([gt_images_latent, cond_images_latent], dim=1)\n",
    "    \n",
    "    return clean_images, total_label, encoder_hidden_states, dropout_mask\n",
    "\n",
    "def decode_latents(vae, latents, scaling_factor=4.0):\n",
    "    \"\"\"Decode latent samples to images using VAE\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Scale latents\n",
    "        latents = latents * 4 / vae.scaling_factor\n",
    "        \n",
    "        # Decode the latents to images\n",
    "        images = vae.decode(latents).sample\n",
    "        \n",
    "        # Normalize images to [0, 1] range\n",
    "        images = (images / 2 + 0.5).clamp(0, 1)\n",
    "        \n",
    "    return images\n",
    "\n",
    "def prepare_conditioning(clip_image=None, cell_line=None, label=None, batch_size=1, device=\"cuda\", weight_dtype=torch.float32):\n",
    "    \"\"\"Prepare conditioning inputs\"\"\"\n",
    "    # Process CLIP image if provided\n",
    "    if clip_image is not None:\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = clip_model(clip_image)\n",
    "    else:\n",
    "        # Create empty encoder hidden states\n",
    "        encoder_hidden_states = torch.zeros((batch_size, 196, 768), device=device, dtype=weight_dtype)\n",
    "    \n",
    "    # Set up cell line and label conditioning\n",
    "    if cell_line is None:\n",
    "        # Create a one-hot vector for cell line (assuming 40 cell lines)\n",
    "        cell_line = torch.zeros((batch_size, 40), device=device, dtype=weight_dtype)\n",
    "        cell_line[:, 0] = 1.0  # Set first cell line as default\n",
    "    \n",
    "    if label is None:\n",
    "        # Create a one-hot vector for label (assuming 13348 labels)\n",
    "        label = torch.zeros((batch_size, 13348), device=device, dtype=weight_dtype)\n",
    "        label[:, 0] = 1.0  # Set first label as default\n",
    "    \n",
    "    total_label = torch.cat([cell_line, label], dim=1)\n",
    "    \n",
    "    return encoder_hidden_states, total_label\n",
    "\n",
    "# def plot_images(images, row_title=None, **kwargs):\n",
    "#     \"\"\"Plot a grid of images\"\"\"\n",
    "#     if not isinstance(images, list):\n",
    "#         images = [images]\n",
    "    \n",
    "#     num_images = len(images)\n",
    "#     fig, axs = plt.subplots(1, num_images, figsize=(12, 12 // num_images))\n",
    "    \n",
    "#     if num_images == 1:\n",
    "#         axs = [axs]\n",
    "    \n",
    "#     for i, img in enumerate(images):\n",
    "#         if isinstance(img, torch.Tensor):\n",
    "#             img = img.detach().cpu().numpy()\n",
    "        \n",
    "#         # Handle different shapes and channel configurations\n",
    "#         if img.ndim == 4 and img.shape[0] == 1:  # [1, C, H, W]\n",
    "#             img = img[0]\n",
    "        \n",
    "#         if img.shape[0] == 3 or img.shape[0] == 1:  # [C, H, W]\n",
    "#             img = img.transpose(1, 2, 0)\n",
    "        \n",
    "#         if img.shape[-1] == 1:  # Single channel\n",
    "#             img = img.squeeze(-1)\n",
    "#             axs[i].imshow(img, cmap='gray')\n",
    "#         else:  # RGB\n",
    "#             axs[i].imshow(img)\n",
    "        \n",
    "#         axs[i].set_xticks([])\n",
    "#         axs[i].set_yticks([])\n",
    "    \n",
    "#     if row_title is not None:\n",
    "#         fig.suptitle(row_title)\n",
    "#     #save the figure\n",
    "#     plt.savefig(f\"{row_title}.png\")\n",
    "#     plt.tight_layout()\n",
    "#     #plt.show()\n",
    "    \n",
    "def plot_images(images, row_title=None, **kwargs):\n",
    "    \"\"\"Plot a grid of images and save to file without displaying\"\"\"\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "    \n",
    "    num_images = len(images)\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(12, 12 // num_images))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().cpu().numpy()\n",
    "        \n",
    "        # Handle different shapes and channel configurations\n",
    "        if img.ndim == 4 and img.shape[0] == 1:  # [1, C, H, W]\n",
    "            img = img[0]\n",
    "        \n",
    "        if img.shape[0] == 3 or img.shape[0] == 1:  # [C, H, W]\n",
    "            img = img.transpose(1, 2, 0)\n",
    "        \n",
    "        if img.shape[-1] == 1:  # Single channel\n",
    "            img = img.squeeze(-1)\n",
    "            axs[i].imshow(img, cmap='gray')\n",
    "        else:  # RGB\n",
    "            axs[i].imshow(img)\n",
    "        \n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "    \n",
    "    if row_title is not None:\n",
    "        fig.suptitle(row_title)\n",
    "    \n",
    "    # Save the figure with appropriate filename\n",
    "    filename = f\"{row_title}.png\" if row_title else \"plot.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # Close the figure to prevent display and free memory\n",
    "    \n",
    "    \n",
    "def save_image(image, output_filename=\"output.png\", **kwargs):\n",
    "    \"\"\"Save a single image to PNG file without using matplotlib\"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Handle torch tensor\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.detach().cpu().numpy()\n",
    "    \n",
    "    # Handle different shapes and channel configurations\n",
    "    if image.ndim == 4 and image.shape[0] == 1:  # [1, C, H, W]\n",
    "        image = image[0]\n",
    "    \n",
    "    if image.shape[0] == 3 or image.shape[0] == 1:  # [C, H, W]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "    \n",
    "    # Handle single channel images\n",
    "    if image.shape[-1] == 1:  # Single channel\n",
    "        image = image.squeeze(-1)\n",
    "        # For grayscale, just use L mode\n",
    "        pil_mode = 'L'\n",
    "    else:  # RGB\n",
    "        pil_mode = 'RGB'\n",
    "    \n",
    "    # Ensure values are in valid range for PIL\n",
    "    if image.max() <= 1.0:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    else:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Create and save the PIL image\n",
    "    if not output_filename.endswith('.png'):\n",
    "        output_filename += '.png'\n",
    "        \n",
    "    pil_img = Image.fromarray(image, mode=pil_mode)\n",
    "    pil_img.save(output_filename)\n",
    "    \n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test dataset to get real conditioning images\n",
    "try:\n",
    "    # test_dataset = FullFieldDataset(\n",
    "    #     data_root='/home/pc/Documents/twisted_diffusion_helper_model/test_images',\n",
    "    #     label_dict='/home/pc/Documents/twisted_diffusion_helper_model/antibody_map.pkl',\n",
    "    #     annotation_dict='/home/pc/Documents/twisted_diffusion_helper_model/annotation_map.pkl'\n",
    "    # )\n",
    "    test_dataset = FullFieldDataset(\n",
    "        data_root='/scratch/groups/emmalu/multimodal_phenotyping/prot_imp/datasets/test_images',\n",
    "        label_dict='/scratch/groups/emmalu/multimodal_phenotyping/prot_imp/datasets/antibody_map.pkl',\n",
    "        annotation_dict='/scratch/groups/emmalu/multimodal_phenotyping/prot_imp/datasets/annotation_map.pkl'\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "    )\n",
    "    # Get a batch of test data\n",
    "    batch = next(iter(test_dataloader))\n",
    "    \n",
    "    # Show conditioning image\n",
    "    cond_images = batch[\"cond_image\"].to(weight_dtype).to(device)\n",
    "    clip_images = batch[\"clip_image\"].to(weight_dtype).to(device)\n",
    "    gt_images = batch[\"gt_image\"].to(weight_dtype).to(device)\n",
    "    # Encode conditioning image to latent space\n",
    "    with torch.no_grad():\n",
    "        #cond_images_latent = prepare_latent_sample(vae, cond_images.repeat(1, 3, 1, 1), weight_dtype)\n",
    "        encoder_hidden_states = clip_model(clip_images)\n",
    "        \n",
    "    # Prepare cell_line and label conditioning\n",
    "    cell_line = batch[\"cell_line\"].to(device).long()\n",
    "    protein_label = batch[\"label\"].to(device).long()\n",
    "    #one hot encoding\n",
    "    #cell_line = torch.nn.functional.one_hot(cell_line, num_classes=40)\n",
    "    #label = torch.nn.functional.one_hot(label, num_classes=13348)\n",
    "    \n",
    "    #total_label = torch.cat([cell_line, label], dim=1)\n",
    "    \n",
    "    # Display conditioning image\n",
    "    for i in range(batch_size):\n",
    "        plot_images(cond_images[i].cpu().float()*0.5+0.5, row_title=f\"generated_images/Testset_Conditioning Image_{i}\")\n",
    "        plot_images(gt_images[i].cpu().float()*0.5+0.5, row_title=f\"generated_images/Testset_Ground Truth Image_{i}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load test dataset: {e}\")\n",
    "    print(\"Generating without real conditioning images...\")\n",
    "    \n",
    "    # Create random conditioning\n",
    "    batch_size = 4\n",
    "    \n",
    "    # Random latent for conditioning (using 8 channels for conditioning)\n",
    "    cond_images_latent = torch.randn(\n",
    "        (batch_size, 8, image_size, image_size),\n",
    "        device=device, \n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    \n",
    "    # Random CLIP embeddings\n",
    "    encoder_hidden_states = torch.randn(\n",
    "        (batch_size, 196, 768),\n",
    "        device=device,\n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    \n",
    "    # Create a one-hot vector for cell line (assuming 40 cell lines)\n",
    "    cell_line = torch.zeros((batch_size, 40), device=device, dtype=weight_dtype)\n",
    "    cell_line[:, 0] = 1.0  # Set first cell line\n",
    "    \n",
    "    # Create a one-hot vector for protein label (assuming 13348 labels)\n",
    "    label = torch.zeros((batch_size, 13348), device=device, dtype=weight_dtype)\n",
    "    label[:, 0] = 1.0  # Set first label\n",
    "    \n",
    "    total_label = torch.cat([cell_line, label], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_edm(\n",
    "    model,\n",
    "    scheduler,\n",
    "    batch_size=1,\n",
    "    image_size=32,\n",
    "    num_inference_steps=50,\n",
    "    condition_latent=None,  # Optional conditioning latent\n",
    "    encoder_hidden_states=None,  # CLIP hidden states\n",
    "    class_labels=None,  # Class labels for conditioning\n",
    "    \n",
    "    protein_labels=None,\n",
    "    cell_line_labels=None,\n",
    "    \n",
    "    guidance_scale=1.0,  # Scale for classifier-free guidance\n",
    "    generator=None,\n",
    "    output_type=\"latent\",  # \"latent\" or \"pt\"\n",
    "):\n",
    "    \"\"\"Sample from the diffusion model using EDM sampling\"\"\"\n",
    "    # Initialize with random noise for the ground truth part\n",
    "    if condition_latent is not None:\n",
    "        latent_channels = 16 - condition_latent.shape[1]  # Get channels for gt part\n",
    "    else:\n",
    "        latent_channels = 16\n",
    "        condition_latent = torch.zeros((batch_size, 0, image_size, image_size), device=device, dtype=weight_dtype)\n",
    "    \n",
    "    # Create random noise for the ground truth part\n",
    "    gt_noise = torch.randn(\n",
    "        (batch_size, latent_channels, image_size, image_size),\n",
    "        generator=generator,\n",
    "        device=device,\n",
    "        dtype=weight_dtype\n",
    "    )\n",
    "    \n",
    "    # Initialize with random noise * sigma_max for the ground truth part\n",
    "    latents = gt_noise * scheduler.sigmas[0].to(device)\n",
    "    \n",
    "    # Set up classifier-free guidance conditioning (if needed)\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "    \n",
    "    # Set up progress bar\n",
    "    progress_bar = tqdm(range(num_inference_steps))\n",
    "    progress_bar.set_description(\"Sampling\")\n",
    "    \n",
    "    # Set timesteps for sampling\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps = scheduler.timesteps\n",
    "    \n",
    "    # Sampling loop\n",
    "    for i, t in enumerate(progress_bar):\n",
    "        # Get sigma for this step\n",
    "        sigma = scheduler.sigmas[i]\n",
    "        sigma_next = scheduler.sigmas[i + 1] if i < len(scheduler.sigmas) - 1 else torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Expand sigma for broadcasting\n",
    "        sigma_expanded = sigma.expand(batch_size).to(device)\n",
    "        sigma_view = sigma_expanded.view(-1, 1, 1, 1).double()\n",
    "        \n",
    "        \n",
    "        latents = latents.double()\n",
    "        \n",
    "        # Combine latents with condition latent\n",
    "        #combined_latent = torch.cat([latents, condition_latent], dim=1)\n",
    "        combined_latent = latents\n",
    "        # Prepare input with noise according to EDM formulation\n",
    "        model_input, timestep_input = edm_clean_image_to_model_input(combined_latent, sigma_view)\n",
    "        timestep_input = timestep_input.squeeze()\n",
    "        \n",
    "        # For classifier-free guidance, we need to do two forward passes:\n",
    "        # one with the conditioning and one without\n",
    "        if do_classifier_free_guidance:\n",
    "            # Unconditional forward pass\n",
    "            model_output_uncond = model(\n",
    "                model_input,\n",
    "                timestep_input,\n",
    "                class_labels=None,  # No class conditioning\n",
    "                encoder_hidden_states=None,  # No CLIP conditioning\n",
    "            ).sample\n",
    "            \n",
    "            # Conditional forward pass\n",
    "            model_output_cond = model(\n",
    "                model_input,\n",
    "                timestep_input,\n",
    "                class_labels=class_labels,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "            ).sample\n",
    "            \n",
    "            # Combine outputs with guidance scale\n",
    "            model_output = model_output_uncond + guidance_scale * (model_output_cond - model_output_uncond)\n",
    "        else:\n",
    "            # Regular conditional forward pass\n",
    "            model.to(weight_dtype)\n",
    "            model_input = model_input.to(weight_dtype)\n",
    "            timestep_input = timestep_input.to(weight_dtype)\n",
    "            #class_labels = class_labels.to(weight_dtype)\n",
    "            protein_labels = protein_labels\n",
    "            cell_line_labels = cell_line_labels\n",
    "\n",
    "            protein_labels = protein_labels.reshape(-1)\n",
    "            cell_line_labels = cell_line_labels.reshape(-1)\n",
    "            encoder_hidden_states = encoder_hidden_states.to(weight_dtype)\n",
    "            model_output = model(\n",
    "                model_input,\n",
    "                timestep_input,\n",
    "                protein_labels = protein_labels,\n",
    "                cell_line_labels = cell_line_labels,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "            ).sample\n",
    "        \n",
    "        # Convert model output to denoised latent (x0 prediction)\n",
    "        pred_x0 = edm_model_output_to_x_0_hat(combined_latent, sigma_view, model_output.double())\n",
    "        \n",
    "        # Extract only the ground truth part (not the conditioning part)\n",
    "        #pred_x0 = pred_x0[:, :latent_channels, :, :]\n",
    "\n",
    "        # Step using the scheduler\n",
    "        # We implement a simplified Euler step for the EDM sampler\n",
    "        denoised = pred_x0\n",
    "        step_sigma = sigma - sigma_next\n",
    "        step_size = step_sigma / sigma\n",
    "        \n",
    "        # Calculate D(xt) (denoised - noisy) / sigma as the \"direction\"\n",
    "        direction = (denoised - latents) / sigma_view\n",
    "        \n",
    "        # Euler step\n",
    "        latents = latents + step_size.item() * sigma_view * direction\n",
    "        \n",
    "        # Add noise if not the last step\n",
    "        # if i < num_inference_steps - 1:\n",
    "        #     noise = torch.randn_like(latents)\n",
    "        #     latents = latents + ((sigma_next**2 - sigma_next**2) ** 0.5) * noise\n",
    "    \n",
    "    # Return the final latents or decode to images based on output_type\n",
    "    if output_type == \"latent\":\n",
    "        return latents\n",
    "    elif output_type == \"pt\":\n",
    "        # Return PyTorch tensor\n",
    "        return latents\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output_type: {output_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 1000/1000 [00:36<00:00, 27.59it/s]\n",
      "/tmp/ipykernel_57156/4242947897.py:27: FutureWarning: Accessing config attribute `scaling_factor` directly via 'AutoencoderKL' object attribute is deprecated. Please access 'scaling_factor' over 'AutoencoderKL's config object instead, e.g. 'unet.config.scaling_factor'.\n",
      "  latents = latents * 4 / vae.scaling_factor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 64 images\n"
     ]
    }
   ],
   "source": [
    "# Generate samples\n",
    "weight_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "num_inference_steps=1000\n",
    "with torch.no_grad():\n",
    "    # Set random seed for reproducibility\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    # Sample from the model\n",
    "    generated_latents = sample_edm(\n",
    "        model=model,\n",
    "        scheduler=scheduler,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        condition_latent=None,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        class_labels=None,\n",
    "        protein_labels = protein_label,\n",
    "        cell_line_labels = cell_line,\n",
    "        guidance_scale=0,\n",
    "        generator=generator,\n",
    "        output_type=\"latent\",\n",
    "    )\n",
    "    \n",
    "    # Decode the latents to images\n",
    "    vae.to(torch.float32)\n",
    "    generated_latents = generated_latents.to(torch.float32)\n",
    "    generated_images_gt = decode_latents(vae, generated_latents[:,:16,:,:])\n",
    "    #generated_images_cond = decode_latents(vae, generated_latents[:,16:,:,:])\n",
    "    \n",
    "print(f\"Generated {len(generated_images_gt)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_images_gt.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated images\n",
    "for i, img in enumerate(generated_images_gt):\n",
    "    plot_images(generated_images_gt[i].cpu(), row_title=f\"generated_images_2/Generated Ground Truth Image_{i}\")\n",
    "    #plot_images(generated_images_cond[i].cpu(), row_title=f\"generated_images/Generated Conditioning Image_{i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_tds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
